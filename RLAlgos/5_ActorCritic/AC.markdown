DQN只学习价值函数，REINFORCE只学习策略函数。

Actor-Critic既学习价值函数，又学习策略函数。其本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。

![Alt text](image.png)
![Alt text](image-1.png)

事实上，用Q值或者V值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。除此之外，REINFORCE算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。

Actor-Critic算法分为两个部分：Actor（策略网络）和Critic（价值网络）
- Actor与环境交互，并在Critic价值函数的指导下用策略梯度学习一个更好的策略
- Critic要做的是通过Actor与环境交互手机的数据学习一个价值函数，价值函数用于判断当前状态什么动作是好的，什么动作不是好的，进而帮助Actor进行更新。
  
![Alt text](image-2.png)

![Alt text](image-3.png)