{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5,  2,  3],\n",
       "       [-1, -1,  0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ptan\n",
    "import numpy as np\n",
    "\n",
    "q_vals = np.array([[5,2,3],[-1,-1,0]])\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions sampled from three prob distributions:\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[0 2 0]\n",
      "[1 2 1]\n",
      "[0 2 1]\n",
      "[1 2 0]\n",
      "[2 2 1]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "print(\"Actions sampled from three prob distributions:\")\n",
    "for _ in range(10):\n",
    "    acts = selector(np.array([\n",
    "        [0.1, 0.8, 0.1],\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.5, 0.5, 0.0]\n",
    "    ]))\n",
    "    print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, actions:int):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.eye(x.size()[0],self.actions)\n",
    "    \n",
    "net = DQNNet(actions=3)\n",
    "net(torch.zeros(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), [None, None])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(model = net, action_selector=selector)\n",
    "agent(torch.zeros(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "agent = ptan.agent.DQNAgent(model = net,action_selector=selector)\n",
    "selector.epsilon = 0.1\n",
    "agent(torch.zeros(10,5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = (x.size()[0], self.actions)\n",
    "        res = torch.zeros(shape, dtype=torch.float32)\n",
    "        res[:,0] = 1\n",
    "        res[:,1] = 1\n",
    "        return res\n",
    "    \n",
    "net = PolicyNet(actions=5)\n",
    "net(torch.zeros(6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 2, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model = net, action_selector=selector, apply_softmax=True)\n",
    "agent(torch.zeros(6,5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class ToyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ToyEnv,self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "    \n",
    "    def step(self, action):\n",
    "        is_done = self.step_index == 10\n",
    "        if is_done:\n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        self.step_index += 1\n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}\n",
    "    \n",
    "#不管是什么都产生相同动作的智能体\n",
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, action: int):\n",
    "        self.action = action\n",
    "\n",
    "    def __call__(self, observations,state):\n",
    "        return [self.action for _ in observations],state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.reset() -> 0\n",
      "env.step(1) -> (1, 1.0, False, {})\n",
      "env.step(2) -> (2, 2.0, False, {})\n",
      "agent: [1, 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m agent \u001b[38;5;241m=\u001b[39m DullAgent(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m exp_source \u001b[38;5;241m=\u001b[39m ptan\u001b[38;5;241m.\u001b[39mexperience\u001b[38;5;241m.\u001b[39mExperienceSource(env\u001b[38;5;241m=\u001b[39menv, agent\u001b[38;5;241m=\u001b[39magent, steps_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexp_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# for idx, exp in enumerate(exp_source):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     if idx > 15:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         break\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     print(exp)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.11/site-packages/ptan/experience.py:69\u001b[0m, in \u001b[0;36mExperienceSource.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_seed)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     obs, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     70\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[1;32m     71\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_count))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "s = env.reset()\n",
    "print(\"env.reset() -> %s\" % s)\n",
    "s = env.step(1)\n",
    "print(\"env.step(1) -> %s\" % str(s))\n",
    "s = env.step(2)\n",
    "print(\"env.step(2) -> %s\" % str(s))\n",
    "\n",
    "# for _ in range(10):\n",
    "#     r = env.step(0)\n",
    "#     print(r)\n",
    "\n",
    "agent = DullAgent(action=1)\n",
    "print(\"agent:\", agent([1, 2],1)[0])\n",
    "\n",
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)\n",
    "next(iter(exp_source))\n",
    "# for idx, exp in enumerate(exp_source):\n",
    "#     if idx > 15:\n",
    "#         break\n",
    "#     print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
